---
layout: retro
title:  "Foglands Retrospective"
image: /assets/img/foglands.png
category: retros
tags: work foglands retro unreal
permalink: /retros/foglands/
---

![Foglands Logo](/assets/img/fog_logo.png)

<h3 style="text-align: center;"><a href="#overview">Overview</a></h3>
<p style = "text-align: center;">General overview of the project, engineering work, and my experience.</p>

<h3 style="text-align: center;"><a href="#features">Feature Breakdowns</a></h3>
<p style = "text-align: center;">Slightly more in depth explanation of various features and tools we made.</p>
* <a href = "#procgen">Procedural Generation</a>
* <a href = "#narrative">Narrative Pipeline</a>
* <a href = "#gas">Gameplay Ability System and Combat</a>
* <a href = "#design">Design Tools</a>
* <a href = "#platformdev">Platform Development</a>
* <a href = "#optimization">Product Considerations and Optimization</a>

<h3 style="text-align: center;"><a href="#lessonslearned">Lessons Learned</a></h3>
<p style = "text-align: center;">Things I learned, what I'd do again, and I wouldn't.</p>  

<h3 style="text-align: center;"><a href="#wrapup">Wrap Up</a></h3>
<p style = "text-align: center;">Closing Thoughts</p>  

<br>
<br>
<h2 style="text-align: left;" id="overview">Overview</h2>

**The Foglands** was a project that contributed greatly to my growth as an engineer, and as a member of development team.  

While I was finishing up my undergrad, I was offered contract work with Well Told Entertainment to help build a roguelike structure for a gameplay demo that had been previously created.
The goal was to see if the mechanics would translate well. After a couple weeks of cleanup and prototyping, we had a pretty nicely contained demo level of a roguelike.  

Shortly after, the project was greenlit by Sony, and I started bring up some core functionality in Unreal while the rest of the team wrapped up another project.
About the time I graduated and began work full time, the team began to roll on and we expanded for full production on the game. We began work on our pre-alpha demo
and were excited to be working with prototypes for PSVR2.  

I got to participate in the hiring process as we expanded the team, and I was promoted into a Lead Engineer role to help organize and plan for the engieering department.
I greatly appreciated the opportunity to be involved in the milestone planning, as well as mentoring and pair programming with associate staff.

Over the course of development, I led the engineers as we built systems for AI, Combat, Interaction, Narrative and more.
> We often paired with designers in strike teams to rapidly prototype new features, abilities, and more. This led to some great team cohesion.

I helped design an engineering growth track, and worked to dedicate time for professional development of the engineering staff, and prototype/experiment time for us to test and refine our tools.
We also worked closely with both internal and external QA, and navigated the certification processes for both PS5 and Quest.

The development of this game took just under three years, and in that time I am really proud of the scope of what we accomplished. 
Simulatneously delivering for multiple platforms, and building a hybrid VR/FPS game were both challenging enough by themselves, and we managed both.

I loved my time working with the team and making Foglands, and I'm excited to see what we all do next!

<h2 style="text-align: left;" id="features">Feature Breakdowns</h2>
<h3 style="text-align: left;" id="procgen">Procedural Generation</h3>
![Procedural Generation](/assets/img/fog_procedural.png)
With the roguelike genre, procedural generation is often utilized for its ability to continue to offer varying experiences through repeated game sessions. We ended up with what I've heard refered to as ***dynamically served*** content.
Instead of the entire world being built from scratch, designers crafted chunks of gameplay experiences that were stitched together based on category and intended level flow. To help out our performance costs, we streamed levels in and out as the player proceded through various airlocks.  

>We utilized the fantastic [SPUD](https://github.com/sinbad/SPUD) plugin from Steve Streeting to save and load various data about objects that the player interacted with in our level chunks so that objects could remain moved, damaged, destroyed, collected and more. I was really happy that I could help fix a few issues with SPUD's clang compilation and contribute back to the project.   

<h3 style="text-align: left;" id="narrative">Narrative Pipeline</h3>
![Foglands Narrative](/assets/vid/fog_narrative.gif)

> A friendly stranger with a once in a lifetime deal?

I'm really proud of the narrative pipeline we built out for this game. We started with the goals of writers being able to write in a format comfortable for them, add data to the lines to help inform the character performance, setup questlines, and utilize variables from the gameplay experience to drive narrative response. We went with [articy:draft3](https://www.articy.com/en/) and it was a fantastic piece of software for this. We used the tools within articy to create line nodes that had information such as 
* Line IDs
* Associated VO files
* suggested emotion
* Variables to wait on for response.

We came up with a human friendly format for our Line ID's that combined writer specified scene codes, a character ID, and the individual line.
> This would have been a bit easier if 16 digit hexadecimal codes were also human friendly.

We then built a plugin that automatically parsed any lines without an ID and created the appropriate one. 
The plugin also checked to see if there was a VO file associated with the line, and used text-to-speech to generate one if not.
All of this information, along with asset status was tracked in a CSV that would be generated for easy tracking of the production status of any given line.
>The team loved robo voice lines. Even as the incredible VO came online, I could still hear the text-to-speech in my mind.

All this data was packaged and imported into Unreal with a commandlet that we wrote to parse the incoming data, and create data assets based on it. 
Here designers could view the line, the narrative suggested emotion (used to drive facial animation), and add in additional animations to be performed when a character read that line.
Another automation step checked to see if there was an animation that matched the Line ID, and would associate it in this data asset as well.
That data was read during runtime to help drive character performance and interaction.
All of this encompassed thousands of lines of dialogue, and hundreds of game state variables. 
> CI and Automation were our best friends.

Finally, we utilized a the "external sources" feature in the WWise Unreal Plugin to use the Line ID to source the file for VO playback. 
Rather than necessitating asset association or lookup, we simply used a data asset and a shared VO event. 
Our dialogue system could now be told to play a scene, would find the appropriate actors, get the data from its lines and would have everything needed for the characters to walk, talk, and animate.

Automating each step of this process really helped allieviate manual processing of assets, and downtime between narrative content creation and its implementation in the game.

<h3 style="text-align: left;" id="gas">Gameplay Ability System & Combat</h3>
![Foglands Combat](/assets/vid/fog_combat.gif)

> Abilities interact with one another, and trigger chain reactions.

The Gameplay Ability System was such a great boon to our development. Both the player and the enemies derived the majority of the combat actions they could take from GAS. Due to the nature of the Roguelike genre, we wanted to have a system that could easily function on event triggers to allow for various actions to trigger one or more abilities and reactions. Using GameplayEvents was a very straightforward and simple use case to do so, and the integration with GAS overall was fantastic.  

After getting our core verbs setup as gameplay tags, and defining basic triggers, it was trivial to create a new ability that got triggered anytime the character recieved that event. 
>One of my favorite scrapped abilities was a bullet modifier that made any bullet fired by the player attach a toxin to an enemy. Once it stacked 3 times, it would explode and spread a stack of toxin to any nearby enemies. When this started expanding onto bullets already chaining to enemies, it quickly showed how conducive our GAS setup was to designing systems that allow for this type of emergent gameplay.  

VR Melee was a fun challenge. One of the common complaints people have is small fast movements that seemingly carry the same force as a haymaker.
We setup calculations that monitored the movement of the players hand and used that to determine if a given strike was valid or not. By checking the average velocity,
spikes in velocity that would be attributed with flailing a controller, and deviation from a trajectory, we could filter out the "spaghetti fighting" and make sure our enemies could defend themselves.
> We also allowed multiple tracking points on an object, so longer objects could actually recieve the correct velocity for the hit location.

We built two systems for AI enemies. One was a utility system that helped make decisions for the character based on weighted curves defined by a designer. 
The other was a more straightforward behavior tree for more "scripted" enemy encounters, such as boss fights. 
We used the UE sense system to help gather sense data, and drive a "threat" value to help transition enemies between states.  

Finally, we used a combat token system to allow for control of how many attacking enemies there could be at once. This allowed designers to specify how much a given action would "cost" and if the enemy could afford it at that time.
Once the enemy completed the attack, it would return the tokens to allow others to attack as well. This helped us dial in how aggresive the enemies felt, and keep the player from being overwhelmed.

<h3 style="text-align: left;" id="design">Design Tools</h3>
To help our designers easily build scripted moments, and interactables experiences in the game, we build a set of scripting tools that allowed for easy chains of events simply through setting a few variables on some actors.
We abstracted things into Event Emitters, Responders, and Actions.  

* Event Emitters were generally used on things like trigger volumes, characters, interactable objects, and would send a signal with an optional payload.

* Event Responders could have one or more emitters associated with them, and would trigger a group of actions. This was helpful to do things like, open a door and turn on a light simultaneously, from any trigger.
They often checked against narrative state, combat state and other global settings to determine if they should respond or not to a given signal.

* Event Actions were the result of a successful response. A simple interface implementation that triggered logic when it recieved a successful response.

> We used SPUD mentioned above to maintain the state of a responders and actions. This helped ensure things like doors that had been opened previously would be in the correct state even after a level is completely streamed out and streamed back in.  

This setup proved to be extremely helpful in easily scripting level interactions without designers needing to dive into and create various blueprints and create a ton of bespoke logic.
Instead, each emitter, responder and action was another tool in the toolkit that could be re-used in another location.

<h3 style="text-align: left;" id="platformdev">Platform Development</h3>
![Foglands Narrative](/assets/vid/fog_product.gif)

> Working with both PS5 and Quest, as well as VR and First Person offered some unique challenges. 

For PSVR2, we started with the prototype hardware and integration into the engine. This was a really cool opportunity to be on the bleeding edge of VR.
We used features like eye tracking to drive target selection, and the advanced haptics for trigger effects and other feedback responses.  

We also made use of the Activities feature to allow players to skip the menus and quickly jump back into a run in progress, or start a new one straight from the system dashboard.

On Quest, we were able to use their Application Space Warping feature to help ensure that we could deliver high quality visuals while still maintaining stable framerates.
Additionally, we setup our CI pipeline to build development and shipping configurations to different channels of our Quest product, allowing for the team to easily have access to the most recent builds simply by updating the headset.
While not everyone of us could have a PS5 Dev Kit and headset, the quest was a great piece of hardware with a small footprint that allowed us all to playtest regularly.

One big win we made for Quest was using the PSO caching process to cache ahead of time and package into the build. 
I wrote  a couple scripts that playtesters could run to enable PSO caching on their device, and retrieve the captured data.
This greatly helped reduce hitching on inital compilation of shaders and keep gameplay smooth.

Needing to support such a vast difference in power between our target platforms, we needed to come up with an easy way to take advantage of each devices full power, and ensure that we delivered visuals consistent with expectation.
One way that we supported that effort was through separation of lighting and rendering effects into different lighting scenarios. 
This allowed us to toggle the appropriate lighting scenario based on the platform, and have the correct light data and effects.

Because our level chunks were created for procedural generation many chunks had multiple configurations to add variety, and each one requierd their own lighting scenario for both the Quest and PS5.
Quickly, we identified the need for automation to assist in cooking the lighting data, and ensuring the correct configuration of sublevels to make sure the data was valid.
I extended the default BuildLighting commandlet to take in additional parameters, and simply toggled the correct configuration.
Then, a simple CI job parsed a CSV generated from the list of level assets and sublevels, and baked each required configuration. 

<h3 style="text-align: left;" id="optimization">Product Considerations and Optimization</h3>
While PSVR2 was our target, we also added a version of the game that could be played entirely in first person without a VR headset, as well as a slightly less graphically demaning version for the Meta Quest.
This simultaneous release helped us work on designing features that could scale up or down based on hardware, and offer the same functionality regardless of input and play type.

VR required great performance to help make sure people weren't getting sick while playing.
That meant that optimization was a priority, as we had framerate targets of at least 72fps on both targets.
We used both in-engine tools like stat counters and Unreal Insights, as well as RenderDoc and various platform tools to help us identify slowdowns in the game.
> Slowdowns were also easy to catch by paying attention to when you suddenly felt sick to your stomach

We captured data, profiled our outliers and set out on optimizing our game.
Some of the first things were standard optimization tasks such as pooling spawns, spreading out loading over time, and identifying high cost rendering.
While others were more in depth cleanups on what we called "tick crimes" where some heavy processing snuck into being done on tick.

>One optimization task that was fun to work through was taking the dot widgets that displayed when you hovered over an item, and pooling them so only a few could exist at any given time.
Rather that all the overdraw, and extra widget cost for all the objects, we had a couple widgets that got checked out by the most relevant interactables. 
It was a great lesson on something that seemed pretty innocuous having a fairly substantial impact.

Hitting our performance targets, while simultaneously delivering high and low end versions, as well as a First Person mode is something I'm very proud of.
We were a small team with a lot of work, and built some awesome stuff.

<h2 style="text-align: left;" id="lessonslearned">Lessons Learned</h2>
![Lessons Learned](/assets/img/fog_lessons.jpg)
### What I learned

Foglands offered me a TON of learning opportunities. From starting by doing rapid prototypes to help fund the project, to growing into a lead role while building out tools and gameplay features.
Being able to take a product from start to finish is such a valuable experience in planning and coordinating development as well as navigating certification and release.  

On a non-technical front, I was glad to have the opportunity to help grow the team through reviewing applications, interviewing, hiring and collaborating with new members.
It was great to see how the team formed and turned into a well oiled machine, as processes were refined and optimized both for the game we were making, and the people making it.
> I really value how this project not only built a game, but a development process as well.

Stepping into a Lead Role was a good expansion of perspective on a project. Instead of just looking to a single feature and the tasks on my Jira board, I was planning for interoperability of features,
setting our milestone goals to ensure we hit our deadlines, and working with not only the engineering team, but collaborating with the team at large to assist and empower their work.

I learned how important it is to have watchful eyes on the scope and deadlines of a project, as the amount of undefined variables can only be limited so much.
Ensuring internal deadlines, regular check-ins, and retrospectives to address issues with deliveries were immensely valuable to releasing a game.

Navigating through optimization and certification was helpful in building the skills of managing workload from internal QA and testing, as well as feedback and bug reports from external QA and certification.

### Things I'd Do Again
- Gameplay Ability System
    - The integration across the engine and flexibility afforded by GAS make it something I plan on using again. The more I learned about and used GAS, the more I came to appreciate it.

- Automation
    - Every minute of time we dedicated to automating slow portions of the workflow paid back 10x. I intend on automating things early and often.

- Design Tools & Narrative Tools
    - As this game was tightly bound between gameplay and narrative, building these tools together helped us ensure we could build the experience we wanted.
    By collaborating directly with design and narrative during this, we were able to build exactly what was needed to make and deliver the content in the format that worked best for each department.

- Encourage and Value Retrospectives
    - Taking a look back at what you've done really helps to identify the positives and negatives of a project.
    By ensuring you address pain points early, you avoid them becoming issues to large to be addressed.
    > Of course I value retrospectives, this page is how long at this point?


### Things I wouldn't do again
- Abilities living on items
    - Early on we had a desire for guns to be able to be modified with items in the world to upgrade and add additional abilities. 
    These abilities were be specific to the weapon, which could be picked up and used by any character in the game (Player and AI)
    While this afforded some flexibility, in a future project, I would opt for ability sets that are granted to the equipping character by the weapon.
    I believe this is more in line with traditional GAS workflow.
    > Due to time and scope constraints, this feature was cut.

- "Porting" features
    - At the beginning of development, there were some features in an old unity prototype that were desired in our Unreal project.
    This unity prototype was more inline with a linear action/adventure game, with none of the roguelike abilities and features.
    We started by porting the logic pretty directly, but quickly re-focused on design intent and doing things in a more "Unreal" manner, and in a way to better fit the roguelike genre.
    Working with design to identify intent and build features to support that intent was the best path forward.

- Waiting to do platform integration.
    - Since we were on prototype hardware, and design was rapidly changing, we focused our efforts on the game itself, and less on the platform.
    However, it was a huge boost to productivity once we had our CI delivering builds directly on to our target devices. Rather than testing in editor and on PC builds for ease, we could play on the actual devices.
    We got much better playtest information as everyone was on target devices, and were able to catch device specific bugs much sooner.

<h2 style="text-align: left;" id="wrapup">Wrap Up</h2>
![Wrap Up](/assets/img/fog_wrapup.jpg)
Well, thats ***The Foglands*** . I was so happy to be a part of this team, and grew a ton during its development.  

Aside from all this work stuff I bought a house, had a kid, turned 30, and started tinkering with new languages, frameworks and hardware.
I baked a bunch of bread, 3D printed a bunch of gizmos, crocheted beanies and blankets, tinkered with an ECS multiplayer project using Rust and Unity, and a whole lot more.
> The data nerd in me loves ECS's. Might have to try one out in a game jam project soon.

I'm excited for what's next and can't wait to start on a new project!

Foglands is available on [Sony PSVR2](https://store.playstation.com/en-us/concept/10003574/) and the [Meta Quest](https://www.meta.com/experiences/5457090074343110/).